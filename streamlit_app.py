import streamlit as st
import lancedb
import os
import json
import time
import sys
import requests
import zipfile
import io

# Function to download and extract database files if they don't exist
def download_database_if_needed():
    db_path = "data/lancedb"
    docling_table_path = f"{db_path}/docling.lance"
    
    if not os.path.exists(docling_table_path) or not os.path.isdir(docling_table_path):
        st.info("Base de datos no encontrada. Descargando archivos...")
        
        # Create directories if they don't exist
        os.makedirs(db_path, exist_ok=True)
        
        # Instructions for manual database setup
        st.markdown("""
        ## Configuraci贸n de la Base de Datos
        
        Para utilizar esta aplicaci贸n completamente, necesitas configurar la base de datos LanceDB localmente:
        
        ### Opci贸n 1: Contacta al administrador
        
        Solicita los archivos de la base de datos al administrador del sistema y col贸calos en la carpeta `data/lancedb/docling.lance`.
        
        ### Opci贸n 2: Descarga la base de datos de Google Drive (pr贸ximamente)
        
        Pronto estar谩 disponible la opci贸n de descargar la base de datos autom谩ticamente.
        """)
        
        # Create a placeholder database with a warning message
        if not os.path.exists(docling_table_path):
            os.makedirs(docling_table_path, exist_ok=True)
            
            # Create a README file explaining the situation
            with open(f"{docling_table_path}/README.txt", "w") as f:
                f.write("Esta es una base de datos de marcador de posici贸n. Por favor, configure la base de datos real para usar la aplicaci贸n completa.")
                
        return False
    return True

# Check if database exists
database_ready = download_database_if_needed()

# Create OpenAI API functions without using the client
def create_embedding(text):
    """Create embedding using OpenAI API directly with requests."""
    if "OPENAI_API_KEY" in st.secrets:
        api_key = st.secrets["OPENAI_API_KEY"]
    else:
        st.error("No se encontr贸 la clave API de OpenAI en los secretos de Streamlit.")
        st.stop()
        
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "input": text,
        "model": "text-embedding-3-small"
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/embeddings",
            headers=headers,
            json=payload
        )
        
        if response.status_code == 200:
            return response.json()["data"][0]["embedding"]
        else:
            st.error(f"Error al crear embedding: {response.status_code}")
            st.error(response.text)
            return None
    except Exception as e:
        st.error(f"Error al crear embedding: {e}")
        return None

def create_chat_completion(messages, temperature=0.7, stream=True):
    """Create chat completion using OpenAI API directly with requests."""
    if "OPENAI_API_KEY" in st.secrets:
        api_key = st.secrets["OPENAI_API_KEY"]
    else:
        st.error("No se encontr贸 la clave API de OpenAI en los secretos de Streamlit.")
        st.stop()
        
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": "gpt-4o-mini",
        "messages": messages,
        "temperature": temperature,
        "stream": stream
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload,
            stream=stream
        )
        
        if response.status_code != 200:
            st.error(f"Error al crear chat completion: {response.status_code}")
            st.error(response.text)
            return None
            
        if stream:
            def generate():
                for line in response.iter_lines():
                    if line:
                        line = line.decode('utf-8')
                        if line.startswith('data: ') and not line.startswith('data: [DONE]'):
                            try:
                                data = json.loads(line[6:])
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except:
                                continue
            
            return generate()
        else:
            return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        st.error(f"Error al crear chat completion: {e}")
        return None

# Initialize LanceDB connection
@st.cache_resource
def init_db():
    """Initialize database connection.

    Returns:
        LanceDB table object
    """
    if not database_ready:
        st.warning("Base de datos no inicializada correctamente. Las funciones de b煤squeda no estar谩n disponibles.")
        return None
        
    try:
        # Check if the database directory exists
        db_path = "data/lancedb"
        if not os.path.exists(db_path):
            st.error(f"隆El directorio de la base de datos {db_path} no existe!")
            return None
            
        # Connect to the database
        db = lancedb.connect(db_path)
        
        # Check if the 'docling' table exists
        tables = db.table_names()
        if "docling" not in tables:
            st.error("隆No se encontr贸 la tabla 'docling'!")
            return None
            
        # Open the table
        return db.open_table("docling")
    except Exception as e:
        st.error(f"Error al conectar a la base de datos: {e}")
        return None

def get_context(query: str, table, num_results: int = 5) -> str:
    """Search the database for relevant context.

    Args:
        query: User's question
        table: LanceDB table object
        num_results: Number of results to return

    Returns:
        str: Concatenated context from relevant chunks with source information
    """
    try:
        # Generate embedding for query using OpenAI
        query_vector = create_embedding(query)
        if not query_vector:
            return "Error al generar embedding para la consulta."
        
        # Search using vector name parameter
        results = table.search(query_vector, vector_column_name="vector").limit(num_results).to_pandas()
        
        # Store search results in session state
        st.session_state.search_results = results
        
        # Just log the count - no expander here
        st.info(f"Se encontraron {len(results)} fuentes relevantes")
        
        contexts = []

        for i, (_, row) in enumerate(results.iterrows()):
            # Get metadata fields
            filename = row["filename"]
            title = row["title"]
            
            # Parse page numbers from string back to list
            try:
                page_numbers = json.loads(row["page_numbers_str"])
            except:
                page_numbers = []
            
            # Build source citation
            source_parts = [filename]
            if page_numbers and len(page_numbers) > 0:
                source_parts.append(f"p. {', '.join(str(p) for p in page_numbers)}")

            source = f"\nFuente: {' - '.join(source_parts)}"
            if title:
                source += f"\nT铆tulo: {title}"

            contexts.append(f"{row['text']}{source}")

        context_text = "\n\n".join(contexts)
        # Store context in session state
        st.session_state.context = context_text
        return context_text
    except Exception as e:
        st.error(f"Error de b煤squeda: {e}")
        return f"Error al recuperar contexto: {str(e)}"

def get_ai_response(messages, context: str, temperature: float = 0.7) -> str:
    """Get streaming response from OpenAI API with fallback options.

    Args:
        messages: Chat history
        context: Retrieved context from database
        temperature: Model temperature (0.0 to 1.0)

    Returns:
        str: Model's response
    """
    system_prompt = f"""# Rol
Eres un asistente experto en pedagog铆a Ignaciana.

# Objetivo
Tu objetivo principal es proporcionar respuestas claras, concretas y detalladas acerca de la pedagog铆a Ignaciana y la propuesta educativa del Colegio San Bartolom茅 La Merced.

# Instrucciones
1. Utiliza 煤nicamente la informaci贸n contenida en tu base de conocimiento interna.
2. Si la pregunta del usuario se relaciona con temas como Paradigma Pedag贸gico Ignaciano (PPI), instrumentos de la Educaci贸n Personalizada u otros conceptos clave de la pedagog铆a Ignaciana, recurre al documento **"Coloquios para un conocimiento pr谩ctico de la propuesta educativa de la Compa帽铆a de Jes煤s"**.
3. Para preguntas sobre la propuesta de innovaci贸n del Colegio San Bartolom茅 La Merced, consulta **"MAGIS_XXI"**.
4. Si la pregunta se relaciona con la obra **"LA PEDAGOGIA IGNACIANA Y SU CARACTER ECLECTICO"**, utiliza ese documento.
5. Cuando se trate de la pedagog铆a Ignaciana a la luz de desarrollos pedag贸gicos contempor谩neos, usa el documento **"Aprender por refracci贸n"**.
6. Para inquietudes acerca del Programa de Afectividad del Colegio San Bartolom茅 La Merced, acude al documento **"Afectividad"**.
7. Si el usuario solicita informaci贸n que no se encuentra en los documentos disponibles, ind铆calo de manera clara: por ejemplo, "No dispongo de informaci贸n suficiente en mi base de conocimiento interna para responder esa pregunta."

# Documentos Disponibles en la Base de Conocimiento
1. **Coloquios para un conocimiento pr谩ctico de la propuesta educativa de la Compa帽铆a de Jes煤s**  
   - Fundamenta la pedagog铆a Ignaciana, el PPI, la Educaci贸n Personalizada y otros conceptos clave.
2. **MAGIS_XXI**  
   - Propuesta de innovaci贸n educativa del Colegio San Bartolom茅 La Merced.
3. **LA PEDAGOGIA IGNACIANA Y SU CARACTER ECLECTICO**
   - Expone la naturaleza ecl茅ctica de la pedagog铆a Ignaciana.
4. **Aprender por refracci贸n**
   - Desarrolla el enfoque de la pedagog铆a Ignaciana en la actualidad con base en estudios pedag贸gicos contempor谩neos.
5. **Afectividad**
   - Describe el Programa de Afectividad del Colegio San Bartolom茅 La Merced.

Contexto del documento sobre el que debes responder:
{context}
"""

    api_messages = [
        {"role": "system", "content": system_prompt}
    ]
    
    # Add the conversation history
    for message in messages:
        api_messages.append(message)
    
    # First, try with gpt-4o-mini
    try:
        print("Intentando usar el modelo gpt-4o-mini")
        
        stream_generator = create_chat_completion(
            messages=api_messages,
            temperature=temperature,
            stream=True
        )
        
        if stream_generator:
            full_response = ""
            for chunk in stream_generator:
                full_response += chunk
                st.write(chunk, end="")
            return full_response
        else:
            raise Exception("No se pudo obtener respuesta del modelo")
    except Exception as primary_error:
        st.warning(f"La llamada a la API principal fall贸: {primary_error}. Intentando m茅todo alternativo...")
        
        try:
            # Try with GPT-3.5
            api_messages[0]["content"] = api_messages[0]["content"] + "\n\nUsa un lenguaje sencillo y claro."
            print("Usando modelo alternativo: gpt-3.5-turbo")
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {st.secrets['OPENAI_API_KEY']}"
            }
            
            payload = {
                "model": "gpt-3.5-turbo",
                "messages": api_messages,
                "temperature": temperature,
                "stream": True
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                stream=True
            )
            
            if response.status_code != 200:
                raise Exception(f"Error {response.status_code}: {response.text}")
                
            def generate():
                for line in response.iter_lines():
                    if line:
                        line = line.decode('utf-8')
                        if line.startswith('data: ') and not line.startswith('data: [DONE]'):
                            try:
                                data = json.loads(line[6:])
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except:
                                continue
            
            full_response = ""
            for chunk in generate():
                full_response += chunk
                st.write(chunk, end="")
            return full_response
            
        except Exception as fallback_error:
            st.error(f"La llamada a la API alternativa tambi茅n fall贸: {fallback_error}")
            return "Encontr茅 un error al intentar generar una respuesta. Por favor, intenta de nuevo m谩s tarde."

# Set up the page
st.set_page_config(
    page_title="Colegio San Bartolom茅 La Merced",
    page_icon="",
    layout="wide",
    initial_sidebar_state="collapsed"  # Changed back to collapsed
)

# Initialize session state for app context and search results
if "first_run" not in st.session_state:
    st.session_state.first_run = True
    # Initialize empty context and search results
    st.session_state.context = ""
    st.session_state.search_results = []
    st.session_state.context_expanded = False

# Initialize other session state variables
if "messages" not in st.session_state:
    st.session_state.messages = []
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7
if "results_count" not in st.session_state:
    st.session_state.results_count = 5

# Initialize database
table = init_db()

# Main content area
st.title(" Colegio San Bartolom茅 La Merced")
st.write("Realice consultas sobre los documentos en nuestra base de conocimiento. Proporcionar茅 respuestas con fuentes relevantes.")

# Simple sidebar with minimal controls
with st.sidebar:
    st.title("Configuraci贸n")
    
    # Clear buttons
    st.header("Acciones")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Limpiar Chat"):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("Limpiar Fuentes"):
            st.session_state.context = ""
            st.session_state.search_results = []
            st.session_state.context_expanded = False
            st.rerun()
    
    # Advanced settings directly in sidebar
    st.header("Configuraci贸n Avanzada")
    st.session_state.temperature = st.slider(
        "Temperatura", 
        min_value=0.0, 
        max_value=1.0, 
        value=st.session_state.temperature,
        step=0.1,
        help="Valores m谩s altos producen respuestas m谩s creativas, valores m谩s bajos producen respuestas m谩s deterministas"
    )
    
    st.session_state.results_count = st.slider(
        "Fuentes a recuperar", 
        min_value=1, 
        max_value=15, 
        value=st.session_state.results_count,
        help="M谩s fuentes proporcionan m谩s contexto pero pueden diluir la relevancia"
    )

# Check if database is ready
if table is None:
    st.warning("La aplicaci贸n est谩 en modo de demostraci贸n. La base de datos completa no est谩 disponible.")
    
    # Display information box with the issue
    st.info("""
    ### Estado de la Aplicaci贸n: Modo de Demostraci贸n
    
    Esta aplicaci贸n requiere una base de datos vectorial LanceDB para funcionar correctamente. 
    
    **Para uso local:**
    1. Descargue la base de datos completa del administrador
    2. Coloque los archivos en la carpeta `data/lancedb/docling.lance`
    3. Reinicie la aplicaci贸n
    
    **Para m谩s informaci贸n:**
    Consulte la documentaci贸n o contacte al administrador del sistema.
    """)
    
    # Show empty chat interface
    st.chat_input("Escribe aqu铆 tu pregunta...", disabled=True)
    st.stop()

# Display existing chat history first
chat_column, sources_column = st.columns([3, 1])

with chat_column:
    # Display all existing messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# Sources are shown in the right column
with sources_column:
    if st.session_state.context and st.session_state.context_expanded:
        st.subheader("Fuentes")
        
        for chunk in st.session_state.context.split("\n\n"):
            # Split into text and metadata parts
            parts = chunk.split("\n")
            text = parts[0]
            metadata = {}
            for line in parts[1:]:
                if ": " in line:
                    key, value = line.split(": ", 1)  # Split on first occurrence only
                    metadata[key] = value

            source = metadata.get("Fuente", "Fuente desconocida")
            title = metadata.get("T铆tulo", "Secci贸n sin t铆tulo")

            st.markdown(
                f"""
                <div class="search-result">
                    <details>
                        <summary>{source}</summary>
                        <div class="metadata">Secci贸n: {title}</div>
                        <div style="margin-top: 8px;">{text}</div>
                    </details>
                </div>
                """,
                unsafe_allow_html=True,
            )

# Chat input at the bottom
if prompt := st.chat_input("Escribe aqu铆 tu pregunta..."):
    # Get relevant context first
    with st.status("Buscando informaci贸n relevante...", expanded=False) as status:
        context = get_context(prompt, table, num_results=st.session_state.results_count)
        st.session_state.context_expanded = True
        
    # Add user message to chat history and display
    st.session_state.messages.append({"role": "user", "content": prompt})
    with chat_column:
        with st.chat_message("user"):
            st.markdown(prompt)
    
    # Generate and display assistant response
    with chat_column:
        with st.chat_message("assistant"):
            response = get_ai_response(
                st.session_state.messages, 
                context,
                temperature=st.session_state.temperature
            )
    
    # Add assistant response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})
    
    # Rerun to update the UI with new sources
    st.rerun()